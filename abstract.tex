\chapter*{Zusammenfassung}
Informationssysteme im Auto werden immer komplexer und werden teils schon mit multimodalen Eingaben wie Sprache, Touch oder Gestik bedient.
Diese multimodalen Interaktionen eröffnen den Nutzern neue Möglichkeiten, die passende Modalität je nach Situation und Eignung zu wählen.
Allerdings muss darauf geachtet werden, die Ablenkung des Fahrers gering zu halten und die Modalitäten in geeigneten Kombinationen zu unterstützen.

Dadurch motiviert erstellen wir in dieser Arbeit ein multimodales Modell, dass Designern multimodaler Schnittstellen helfen soll, die Interaktionszeiten verschiedener Modalitäten vorherzusagen und zu vergleichen.
Dazu entwickeln wir einen Prototypen, der mit Touch, Geste und Sprache bedient werden kann.
Mittels eines within-subject Design testen 22 Probanden in vier Anwendungsbeispielen alle Kombinationen von Modalitäten mit einem Moduswechsel, sowie die entsprechenden unimodalen Varianten.
Aus diesen Interaktionszeiten leiten wir ein Modell mit verschiedenen Aktionszeiten für die Modalitäten Touch, Sprache und Geste ab, sowie deren Wechselkosten untereinander.
Unser Modell lehnt sich an das Konzept des Keystroke-Level Modells und Erweiterungen an.
Wir konzentrieren uns jedoch nicht auf einzelne Operatoren, sondern auf grob körnigere Aktionen und deren Wechselkosten.

Zur Validierung des multimodalen Modells werden in einer weiteren Studie fünf verschiedene Aufgaben mit bis zu vier Moduswechseln untersucht und mit unserer Vorhersage des Modells verglichen.
Es zeigte sich, dass unser Modell multimodale Interaktionen mit einem durchschnittlichen Vorhersagefehler von 14,746\% vorhersagen konnte
Diese Abschätzung von multimodalen Interaktionszeiten im Auto ermöglicht es Designern bereits in einem frühen Stadium der Entwicklung zu unterstützen und hilft ihnen die Ablenkung des Fahrers zu minimieren.


%\selectlanguage{english}
\chapter*{Abstract}
In-Vehicle-Information-Systems are getting more and more complex and the possibilities for using multimodal interactions like speech, touch and gestures are growing.
A big advantage of multimodal interactions is that a user can decide in which situation he wants to use the most suitable kind of interaction.
ON the other hand such a multimodal system should minimize the distraction of the driver and support the best combinations of modalities.

Motivated by this we design in this thesis a multimodal model to support designers of multimodal interfaces by predicting the interaction time of such systems.
By comparing interaction times, the best combinations can be detected and a favorable design is possible.
Our model is similar to the concept of the keystroke-level-model and its extensions.
However we focus on actions for each modality and the resulting change costs instead of single operators.
With the measured interaction times of our first study we develope a model with different times for each modality (touch, speech and gestures).
We also model the changing costs between one modality to another.

To validate our multimodal model we investigate in a second study five tasks and compared the total task time with the prediction time of our model.
We show that the prediction time of our model is a good match compared to the observed times.
The average RMSE was 14,746\%.
These predictions can support designer in an early stage of implementation and help them to reduce driver distraction.