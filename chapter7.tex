\chapter{Zusammenfassung und Ausblick}\label{cha:Zusamenfassung}
Ausgehend von den vergangenen und aktuellen Forschungen in Bereichen der Informationssysteme in Fahrzeugen, multimodalen Interaktionen und Modellen zur Bestimmung von Bedienzeiten, wurde in dieser Arbeit ein multimodales Modell vorgestellt, mit deren Hilfe multimodale Interaktionszeiten abgeschätzt und Vergleiche zwischen Kombinationen aus Modalitäten getroffen werden können.

Motiviert wurden wir durch die Vorteile verschiedener Modalitäten und die damit verbundene Möglichkeit, den Nutzer zu entlasten und ihn somit weniger von der Fahraufgabe abzulenken.
Der Fahrer kann selbst entscheiden, in welchen Situationen, welche Modalität für den jeweiligen Schritt am geeignetsten und ungefährlichsten ist, was nach \citet{Muller_2011} einen wesentlichen Vorteil in Autos darstellt.

Im Rahmen dieser Arbeit sollte unter anderem erforscht werden, wie multimodale Interaktionen im Fahrzeug aussehen, in welche Aktionen sich diese gliedern lassen und welche Umsetzungsmöglichkeiten sich für die Modalitäten von Haptik, Touch, Geste und Sprache anbieten.
In einem Workshop wurden dazu diverse Anwendungsbeispiele in vereinfachte Aktionen gegliedert und Umsetzungsmöglichkeiten dieser gesammelt.
Auf dieser Grundlage wurde ein Konzept für ein multimodales Modell entworfen und in einem multimodalen Prototypen umgesetzt, der mit Touch-, Gesten- und Spracheingabe bedient werden kann.

Die Basis des implementierten Prototyp bildeten dabei ein Surface zur Erkennung von Touch-Eingaben, ein Leap Motion Controller, um die  Gestenerkennung zu ermöglichen und das Abgreifen definierter Wörtern mit Hilfe der in Windows integrierten Spracherkennung.  
Diese wurden in einem BMW 6er Gran Coupé angebracht, um den Interaktionsraum realistisch abzubilden. 

Das Touchdisplay wurde vor das ursprüngliche Display angebracht und der Leap Motion Controller befand sich zwischen Gangschaltung und Dashboard. 
Der Interaktionsbereich für Gesten befand sich in dem Dreiecks-Bereich von Lenkrad, Rückspiegel und Gangschaltung, der von \citet{Riener:2013:SIG} als häufigster Interaktionsbereich identifiziert wurde.
Für die Benutzerstudie wurde in vier Anwendungsbeispielen alle Kombinationen der Modalitäten Touch, Geste und Sprache mit einem Modalitätswechsel von jedem der 22 Probanden in zwei Messdurchgängen gemessen. 
Um Lerneffekte auszugleichen wurden die Kombinationen permutiert.

Aus dieser Studie konnten wir verschiedene Aktionszeiten für jede Modalität ermitteln und daraus unser multimodales Modell ableiten, welches unimodale Aktionszeiten und deren Wechselkosten abbildet. 
Zusätzlich wurde die Eignung und Gefallen der Kombinationen abgefragt, sowie die Eignung der Modalitäten für die verschiedenen Screentypen.

Um das multimodale Modell zu validieren wurde in einer zweiten Benutzerstudie, der Prototyp für neue Anwendungsbeispiele angepasst, die bis zu vier Modalitätswechsel enthalten.
Die Gesamtdauer der Anwendungsbeispiele wurde mit den, aus dem Modell vorhergesagten, Interaktionszeiten verglichen. 
Mit einem durchschnittlichen Vorhersagefehler von 14,746\% lagen unsere Ergebnisse in den empfohlenen Bereichen von 5-30\% \citep{Card_1980, Luo_2005,Teo:2006}. 

Mit der Entwicklung unseres Modells zur Vorhersage von multimodalen Interaktionszeiten ist es uns gelungen Designern die Möglichkeit zu geben, bereits in einem frühem Stadium der Entwicklungsphase multimodale Interaktionen einzuschätzen und zu vergleichen. 
Damit können in geeigneter Weise die besten potenziellen Kombinationen unterstützt werden, um dem Fahrer auch die besten Möglichkeiten in verständlicher Weise anzuzeigen. 
Des Weiteren wird auch die visuelle und mentale Beanspruchung für den Fahrer so gering wie möglich gehalten. 

Ob eine Variante besonders gut oder schlecht ist hängt von der Situation ab, weshalb ein seriell redundantes multimodales IVIS vorteilhaft ist, um in jedem Schritt den Fahrer selbst die beste Modalität wählen zu lassen. 
Beispielsweis ist die Eingabe eines Zieles per Touch im stehenden Auto eine gute Variante, die jedoch während der Fahrt den Fahrer durch die längere Interaktion zu sehr ablenken kann. 
In diesem Fall stellt die Texteingabe per Sprache eine geeignete Alternative dar.

Unter Betrachtung der Übertragbarkeit wurden diverse Optimierungen unseres multimodalen Modells erläutert.
Unser Modell zur Vorhersage von multimodalen Interaktionen im IVIS soll in Zukunft noch dahingehend weiterentwickelt werden auch haptische Bedienelemente mit Aktionszeiten miteinzubeziehen. 
Des weiteren soll das Modell auf weitere Gesten erweitert werden. 
Außerdem ist eine detailliertere Untersuchungen von Sprachbefehle bis hin zur Ermittlung von gesprochenen Sätzen ein wichtiger Schritt, um die Interaktionszeiten für zukünftige multimodale Interfaces besser vorhersagen zu können.

Die Validierung unseres Modell wurde nur für einige Aktionen vorgenommen, da wir die Studiendauer in einem Rahmen von 45 Minuten halten wollten. 
Statt alle Aktionen zu testen, einigten wir uns auf fünf Anwendungsbeispiele, die uns als realistische Repräsentation üblicher Anwendungen erschien.
Dabei wurden unter anderem die Einschätzungen der Probanden zur Eignung von Kombinationen der Modalitäten zur Rate gezogen (siehe \fref{fig:Smiley_Eignung_Gefallen}).
Eine komplette Validierung des multimodalen Modells, sollte jedoch noch vorgenommen werden. 

Aus den zwei Studien konnte beobachtet werde, das beide Bereiche für einige Nutzer zu weit entfernt positioniert waren.   
Für eine bessere Bedienbarkeit und zur Reduktion der Fahrerablenkung, sollten die Interaktionsbereiche zur Touch- und Gesten-Eingabe auf ihre Erreichbarkeit verbessert werden. 
